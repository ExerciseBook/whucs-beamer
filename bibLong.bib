
@inproceedings{devlin_bert_2019,
	title = {{BERT}: {Pre}-training of {Deep} {Bidirectional} {Transformers} for {Language} {Understanding}},
	url = {https://doi.org/10.18653/v1/n19-1423},
	doi = {10.18653/v1/n19-1423},
	booktitle = {Proceedings of the 2019 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}, {NAACL}-{HLT} 2019, {Minneapolis}, {MN}, {USA}, {June} 2-7, 2019, {Volume} 1 ({Long} and {Short} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
	editor = {Burstein, Jill and Doran, Christy and Solorio, Thamar},
	year = {2019},
	pages = {4171--4186},
}

@inproceedings{real_regularized_2019,
	title = {Regularized {Evolution} for {Image} {Classifier} {Architecture} {Search}},
	url = {https://doi.org/10.1609/aaai.v33i01.33014780},
	doi = {10.1609/aaai.v33i01.33014780},
	booktitle = {The {Thirty}-{Third} {AAAI} {Conference} on {Artificial} {Intelligence}, {AAAI} 2019, {The} {Thirty}-{First} {Innovative} {Applications} of {Artificial} {Intelligence} {Conference}, {IAAI} 2019, {The} {Ninth} {AAAI} {Symposium} on {Educational} {Advances} in {Artificial} {Intelligence}, {EAAI} 2019, {Honolulu}, {Hawaii}, {USA}, {January} 27 - {February} 1, 2019},
	publisher = {AAAI Press},
	author = {Real, Esteban and Aggarwal, Alok and Huang, Yanping and Le, Quoc V.},
	year = {2019},
	pages = {4780--4789},
}

@article{ouyang_communication_2021,
	title = {Communication optimization strategies for distributed deep neural network training: {A} survey},
	volume = {149},
	url = {https://doi.org/10.1016/j.jpdc.2020.11.005},
	doi = {10.1016/j.jpdc.2020.11.005},
	journal = {J. Parallel Distributed Comput.},
	author = {Ouyang, Shuo and Dong, Dezun and Xu, Yemao and Xiao, Liquan},
	year = {2021},
	pages = {52--65},
}

@inproceedings{huang_gpipe_2019,
	title = {{GPipe}: {Efficient} {Training} of {Giant} {Neural} {Networks} using {Pipeline} {Parallelism}},
	url = {https://proceedings.neurips.cc/paper/2019/hash/093f65e080a295f8076b1c5722a46aa2-Abstract.html},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 32: {Annual} {Conference} on {Neural} {Information} {Processing} {Systems} 2019, {NeurIPS} 2019, {December} 8-14, 2019, {Vancouver}, {BC}, {Canada}},
	author = {Huang, Yanping and Cheng, Youlong and Bapna, Ankur and Firat, Orhan and Chen, Dehao and Chen, Mia Xu and Lee, HyoukJoong and Ngiam, Jiquan and Le, Quoc V. and Wu, Yonghui and Chen, Zhifeng},
	editor = {Wallach, Hanna M. and Larochelle, Hugo and Beygelzimer, Alina and d'Alché-Buc, Florence and Fox, Emily B. and Garnett, Roman},
	year = {2019},
	pages = {103--112},
}

@inproceedings{li_terapipe_2021,
	series = {Proceedings of {Machine} {Learning} {Research}},
	title = {{TeraPipe}: {Token}-{Level} {Pipeline} {Parallelism} for {Training} {Large}-{Scale} {Language} {Models}},
	volume = {139},
	url = {http://proceedings.mlr.press/v139/li21y.html},
	booktitle = {Proceedings of the 38th {International} {Conference} on {Machine} {Learning}, {ICML} 2021, 18-24 {July} 2021, {Virtual} {Event}},
	publisher = {PMLR},
	author = {Li, Zhuohan and Zhuang, Siyuan and Guo, Shiyuan and Zhuo, Danyang and Zhang, Hao and Song, Dawn and Stoica, Ion},
	editor = {Meila, Marina and Zhang, Tong},
	year = {2021},
	pages = {6543--6552},
}

@inproceedings{narayanan_pipedream_2019,
	title = {{PipeDream}: generalized pipeline parallelism for {DNN} training},
	url = {https://doi.org/10.1145/3341301.3359646},
	doi = {10.1145/3341301.3359646},
	booktitle = {Proceedings of the 27th {ACM} {Symposium} on {Operating} {Systems} {Principles}, {SOSP} 2019, {Huntsville}, {ON}, {Canada}, {October} 27-30, 2019},
	publisher = {ACM},
	author = {Narayanan, Deepak and Harlap, Aaron and Phanishayee, Amar and Seshadri, Vivek and Devanur, Nikhil R. and Ganger, Gregory R. and Gibbons, Phillip B. and Zaharia, Matei},
	editor = {Brecht, Tim and Williamson, Carey},
	year = {2019},
	pages = {1--15},
}

@inproceedings{he_deep_2016,
	title = {Deep {Residual} {Learning} for {Image} {Recognition}},
	url = {https://doi.org/10.1109/CVPR.2016.90},
	doi = {10.1109/CVPR.2016.90},
	booktitle = {2016 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}, {CVPR} 2016, {Las} {Vegas}, {NV}, {USA}, {June} 27-30, 2016},
	publisher = {IEEE Computer Society},
	author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	year = {2016},
	pages = {770--778},
}

@inproceedings{liu_swin_2021,
	title = {Swin {Transformer}: {Hierarchical} {Vision} {Transformer} using {Shifted} {Windows}},
	url = {https://doi.org/10.1109/ICCV48922.2021.00986},
	doi = {10.1109/ICCV48922.2021.00986},
	booktitle = {2021 {IEEE}/{CVF} {International} {Conference} on {Computer} {Vision}, {ICCV} 2021, {Montreal}, {QC}, {Canada}, {October} 10-17, 2021},
	publisher = {IEEE},
	author = {Liu, Ze and Lin, Yutong and Cao, Yue and Hu, Han and Wei, Yixuan and Zhang, Zheng and Lin, Stephen and Guo, Baining},
	year = {2021},
	pages = {9992--10002},
}

@inproceedings{dean_large_2012,
	title = {Large {Scale} {Distributed} {Deep} {Networks}},
	url = {https://proceedings.neurips.cc/paper/2012/hash/6aca97005c68f1206823815f66102863-Abstract.html},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 25: 26th {Annual} {Conference} on {Neural} {Information} {Processing} {Systems} 2012. {Proceedings} of a meeting held {December} 3-6, 2012, {Lake} {Tahoe}, {Nevada}, {United} {States}},
	author = {Dean, Jeffrey and Corrado, Greg and Monga, Rajat and Chen, Kai and Devin, Matthieu and Le, Quoc V. and Mao, Mark Z. and Ranzato, Marc'Aurelio and Senior, Andrew W. and Tucker, Paul A. and Yang, Ke and Ng, Andrew Y.},
	editor = {Bartlett, Peter L. and Pereira, Fernando C. N. and Burges, Christopher J. C. and Bottou, Léon and Weinberger, Kilian Q.},
	year = {2012},
	pages = {1232--1240},
}

@inproceedings{gupta_training_2021,
	title = {Training {Recommender} {Systems} at {Scale}: {Communication}-{Efficient} {Model} and {Data} {Parallelism}},
	url = {https://doi.org/10.1145/3447548.3467080},
	doi = {10.1145/3447548.3467080},
	booktitle = {{KDD} '21: {The} 27th {ACM} {SIGKDD} {Conference} on {Knowledge} {Discovery} and {Data} {Mining}, {Virtual} {Event}, {Singapore}, {August} 14-18, 2021},
	publisher = {ACM},
	author = {Gupta, Vipul and Choudhary, Dhruv and Tang, Ping Tak Peter and Wei, Xiaohan and Wang, Xing and Huang, Yuzhen and Kejariwal, Arun and Ramchandran, Kannan and Mahoney, Michael W.},
	editor = {Zhu, Feida and Ooi, Beng Chin and Miao, Chunyan},
	year = {2021},
	pages = {2928--2936},
}

@inproceedings{murakami_simp_1989,
	title = {{SIMP} ({Single} {Instruction} stream/{Multiple} {Instruction} {Pipelining}): {A} {Novel} {High}-{Speed} {Single}-{Processor} {Architecture}},
	url = {https://doi.org/10.1145/74925.74935},
	doi = {10.1145/74925.74935},
	booktitle = {Proceedings of the 16th {Annual} {International} {Symposium} on {Computer} {Architecture}. {Jerusalem}, {Israel}, {June} 1989},
	publisher = {ACM},
	author = {Murakami, Kazuaki J. and Irie, Naohiko and Kuga, Morihiro and Tomita, Shinji},
	editor = {Syre, Jean-Claude},
	year = {1989},
	pages = {78--85},
}

@article{kim_torchgpipe_2020,
	title = {torchgpipe: {On}-the-fly {Pipeline} {Parallelism} for {Training} {Giant} {Models}},
	volume = {abs/2004.09910},
	url = {https://arxiv.org/abs/2004.09910},
	journal = {CoRR},
	author = {Kim, Chiheon and Lee, Heungsub and Jeong, Myungryong and Baek, Woonhyuk and Yoon, Boogeon and Kim, Ildoo and Lim, Sungbin and Kim, Sungwoong},
	year = {2020},
	note = {arXiv: 2004.09910},
}

@inproceedings{redmon_you_2016,
	title = {You {Only} {Look} {Once}: {Unified}, {Real}-{Time} {Object} {Detection}},
	url = {https://doi.org/10.1109/CVPR.2016.91},
	doi = {10.1109/CVPR.2016.91},
	booktitle = {2016 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}, {CVPR} 2016, {Las} {Vegas}, {NV}, {USA}, {June} 27-30, 2016},
	publisher = {IEEE Computer Society},
	author = {Redmon, Joseph and Divvala, Santosh Kumar and Girshick, Ross B. and Farhadi, Ali},
	year = {2016},
	pages = {779--788},
}

@inproceedings{li_chimera_2021,
	title = {Chimera: efficiently training large-scale neural networks with bidirectional pipelines},
	url = {https://doi.org/10.1145/3458817.3476145},
	doi = {10.1145/3458817.3476145},
	booktitle = {{SC} '21: {The} {International} {Conference} for {High} {Performance} {Computing}, {Networking}, {Storage} and {Analysis}, {St}. {Louis}, {Missouri}, {USA}, {November} 14 - 19, 2021},
	publisher = {ACM},
	author = {Li, Shigang and Hoefler, Torsten},
	editor = {Supinski, Bronis R. de and Hall, Mary W. and Gamblin, Todd},
	year = {2021},
	pages = {27:1--27:14},
}

@article{li_pytorch_2020,
	title = {{PyTorch} {Distributed}: {Experiences} on {Accelerating} {Data} {Parallel} {Training}},
	volume = {13},
	url = {http://www.vldb.org/pvldb/vol13/p3005-li.pdf},
	doi = {10.14778/3415478.3415530},
	number = {12},
	journal = {Proc. VLDB Endow.},
	author = {Li, Shen and Zhao, Yanli and Varma, Rohan and Salpekar, Omkar and Noordhuis, Pieter and Li, Teng and Paszke, Adam and Smith, Jeff and Vaughan, Brian and Damania, Pritam and Chintala, Soumith},
	year = {2020},
	pages = {3005--3018},
}

@inproceedings{paine_gpu_2014,
	title = {{GPU} {Asynchronous} {Stochastic} {Gradient} {Descent} to {Speed} {Up} {Neural} {Network} {Training}},
	url = {http://arxiv.org/abs/1312.6186},
	booktitle = {2nd {International} {Conference} on {Learning} {Representations}, {ICLR} 2014, {Banff}, {AB}, {Canada}, {April} 14-16, 2014, {Workshop} {Track} {Proceedings}},
	author = {Paine, Thomas and Jin, Hailin and Yang, Jianchao and Lin, Zhe and Huang, Thomas S.},
	editor = {Bengio, Yoshua and LeCun, Yann},
	year = {2014},
}

@article{chen_revisiting_2016,
	title = {Revisiting {Distributed} {Synchronous} {SGD}},
	volume = {abs/1604.00981},
	url = {http://arxiv.org/abs/1604.00981},
	journal = {CoRR},
	author = {Chen, Jianmin and Monga, Rajat and Bengio, Samy and Józefowicz, Rafal},
	year = {2016},
	note = {arXiv: 1604.00981},
}

@inproceedings{alistarh_convergence_2018,
	title = {The {Convergence} of {Sparsified} {Gradient} {Methods}},
	url = {https://proceedings.neurips.cc/paper/2018/hash/314450613369e0ee72d0da7f6fee773c-Abstract.html},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 31: {Annual} {Conference} on {Neural} {Information} {Processing} {Systems} 2018, {NeurIPS} 2018, {December} 3-8, 2018, {Montréal}, {Canada}},
	author = {Alistarh, Dan and Hoefler, Torsten and Johansson, Mikael and Konstantinov, Nikola and Khirirat, Sarit and Renggli, Cédric},
	editor = {Bengio, Samy and Wallach, Hanna M. and Larochelle, Hugo and Grauman, Kristen and Cesa-Bianchi, Nicolò and Garnett, Roman},
	year = {2018},
	pages = {5977--5987},
}

@inproceedings{recht_hogwild_2011,
	title = {Hogwild: {A} {Lock}-{Free} {Approach} to {Parallelizing} {Stochastic} {Gradient} {Descent}},
	url = {https://proceedings.neurips.cc/paper/2011/hash/218a0aefd1d1a4be65601cc6ddc1520e-Abstract.html},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 24: 25th {Annual} {Conference} on {Neural} {Information} {Processing} {Systems} 2011. {Proceedings} of a meeting held 12-14 {December} 2011, {Granada}, {Spain}},
	author = {Recht, Benjamin and Ré, Christopher and Wright, Stephen J. and Niu, Feng},
	editor = {Shawe-Taylor, John and Zemel, Richard S. and Bartlett, Peter L. and Pereira, Fernando C. N. and Weinberger, Kilian Q.},
	year = {2011},
	pages = {693--701},
}

@inproceedings{stich_sparsified_2018,
	title = {Sparsified {SGD} with {Memory}},
	url = {https://proceedings.neurips.cc/paper/2018/hash/b440509a0106086a67bc2ea9df0a1dab-Abstract.html},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 31: {Annual} {Conference} on {Neural} {Information} {Processing} {Systems} 2018, {NeurIPS} 2018, {December} 3-8, 2018, {Montréal}, {Canada}},
	author = {Stich, Sebastian U. and Cordonnier, Jean-Baptiste and Jaggi, Martin},
	editor = {Bengio, Samy and Wallach, Hanna M. and Larochelle, Hugo and Grauman, Kristen and Cesa-Bianchi, Nicolò and Garnett, Roman},
	year = {2018},
	pages = {4452--4463},
}

@inproceedings{bottou_large-scale_2010,
	title = {Large-{Scale} {Machine} {Learning} with {Stochastic} {Gradient} {Descent}},
	url = {https://doi.org/10.1007/978-3-7908-2604-3\_16},
	doi = {10.1007/978-3-7908-2604-3_16},
	booktitle = {19th {International} {Conference} on {Computational} {Statistics}, {COMPSTAT} 2010, {Paris}, {France}, {August} 22-27, 2010 - {Keynote}, {Invited} and {Contributed} {Papers}},
	publisher = {Physica-Verlag},
	author = {Bottou, Léon},
	editor = {Lechevallier, Yves and Saporta, Gilbert},
	year = {2010},
	pages = {177--186},
}

@misc{noauthor_foldinghome_nodate,
	title = {Folding@home {Assembles} an {Exaflop} to {Fight} {COVID}-19 {\textbar} {NVIDIA} {Blog}},
	url = {https://blogs.nvidia.com/blog/2020/04/01/foldingathome-exaflop-coronavirus/},
	urldate = {2022-04-20},
	file = {Folding@home Assembles an Exaflop to Fight COVID-19 | NVIDIA Blog:C\:\\Users\\86159\\Zotero\\storage\\HK5ZDG7T\\foldingathome-exaflop-coronavirus.html:text/html},
}

@misc{noauthor_kaggle_nodate,
	title = {Kaggle: {Your} {Home} for {Data} {Science}},
	url = {https://www.kaggle.com/},
	urldate = {2022-04-20},
	file = {Kaggle\: Your Home for Data Science:C\:\\Users\\86159\\Zotero\\storage\\IBJKXIGS\\www.kaggle.com.html:text/html},
}

@misc{noauthor_pytorch_nodate,
	title = {{PyTorch}},
	url = {https://www.pytorch.org},
	abstract = {An open source machine learning framework that accelerates the path from research prototyping to production deployment.},
	language = {en},
	urldate = {2022-04-20},
}

@inproceedings{diskin_distributed_2021,
	title = {Distributed {Deep} {Learning} {In} {Open} {Collaborations}},
	volume = {34},
	url = {https://proceedings.neurips.cc/paper/2021/hash/41a60377ba920919939d83326ebee5a1-Abstract.html},
	abstract = {Modern deep learning applications require increasingly more compute to train state-of-the-art models. To address this demand, large corporations and institutions use dedicated High-Performance Computing clusters, whose construction and maintenance are both environmentally costly and well beyond the budget of most organizations. As a result, some research directions become the exclusive domain of a few large industrial and even fewer academic actors. To alleviate this disparity, smaller groups may pool their computational resources and run collaborative experiments that benefit all participants. This paradigm, known as grid- or volunteer computing, has seen successful applications in numerous scientific areas. However, using this approach for machine learning is difficult due to high latency, asymmetric bandwidth, and several challenges unique to volunteer computing. In this work, we carefully analyze these constraints and propose a novel algorithmic framework designed specifically for collaborative training. We demonstrate the effectiveness of our approach for SwAV and ALBERT pretraining in realistic conditions and achieve performance comparable to traditional setups at a fraction of the cost. Finally, we provide a detailed report of successful collaborative language model pretraining with nearly 50 participants.},
	urldate = {2022-04-20},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Diskin, Michael and Bukhtiyarov, Alexey and Ryabinin, Max and Saulnier, Lucile and lhoest, quentin and Sinitsin, Anton and Popov, Dmitry and Pyrkin, Dmitry V. and Kashirin, Maxim and Borzunov, Alexander and Villanova del Moral, Albert and Mazur, Denis and Kobelev, Ilia and Jernite, Yacine and Wolf, Thomas and Pekhimenko, Gennady},
	year = {2021},
	pages = {7879--7897},
	file = {Full Text PDF:C\:\\Users\\86159\\Zotero\\storage\\3P89RMEE\\Diskin 等。 - 2021 - Distributed Deep Learning In Open Collaborations.pdf:application/pdf},
}

@inproceedings{aviv_asynchronous_2021,
	title = {Asynchronous {Distributed} {Learning} : {Adapting} to {Gradient} {Delays} without {Prior} {Knowledge}},
	shorttitle = {Asynchronous {Distributed} {Learning}},
	url = {https://proceedings.mlr.press/v139/aviv21a.html},
	abstract = {We consider stochastic convex optimization problems, where several machines act asynchronously in parallel while sharing a common memory. We propose a robust training method for the constrained setting and derive non asymptotic convergence guarantees that do not depend on prior knowledge of update delays, objective smoothness, and gradient variance. Conversely, existing methods for this setting crucially rely on this prior knowledge, which render them unsuitable for essentially all shared-resources computational environments, such as clouds and data centers. Concretely, existing approaches are unable to accommodate changes in the delays which result from dynamic allocation of the machines, while our method implicitly adapts to such changes.},
	language = {en},
	urldate = {2022-04-20},
	booktitle = {Proceedings of the 38th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Aviv, Rotem Zamir and Hakimi, Ido and Schuster, Assaf and Levy, Kfir Yehuda},
	month = jul,
	year = {2021},
	note = {ISSN: 2640-3498},
	pages = {436--445},
	file = {Full Text PDF:C\:\\Users\\86159\\Zotero\\storage\\U4NH43KQ\\Aviv 等。 - 2021 - Asynchronous Distributed Learning  Adapting to Gr.pdf:application/pdf;Supplementary PDF:C\:\\Users\\86159\\Zotero\\storage\\9SFV8N4R\\Aviv 等。 - 2021 - Asynchronous Distributed Learning  Adapting to Gr.pdf:application/pdf},
}

@article{__2015,
	title = {深度学习研究综述},
	number = {1},
	journal = {北京工业大学学报},
	author = {{尹宝才} and {王文通} and {王立春}},
	month = jan,
	year = {2015},
	pages = {48--59},
}

@phdthesis{__2014,
	type = {硕士论文},
	title = {基于深度学习的中文自然语言处理},
	author = {{吴轲}},
	year = {2014},
}

@article{__2016,
	title = {深度卷积神经网络在计算机视觉中的应用研究综述},
	volume = {31},
	number = {1},
	journal = {数据采集与处理},
	author = {{卢宏涛} and {张秦川}},
	month = jan,
	year = {2016},
	pages = {1--17},
}

@article{__2020,
	title = {基于深度卷积残差学习的图像超分辨},
	volume = {52},
	url = {https://d.wanfangdata.com.cn/periodical/ChlQZXJpb2RpY2FsQ0hJTmV3UzIwMjIwNDE1Eg96emR4eGIyMDIwMDMwMDcaCHBzN256dnpm},
	abstract = {传统的超分辨率卷积神经网络难以获得丰富的细节和边缘信息.提出了一种多映射残差卷积神经网络(MMRCNN)来解决这些问题.具体来说,MMRCNN直接使用低分辨率图像作为网络的初始输入,然后使用卷积层提取特征.其次,通过残差学习构建多映射网络,添加批量归一化层优化网络,使聚合高分辨率图像时所需要的特征信息能够变得极为丰富.最后,使用反卷积层来完成图像上采样,输出高分辨率图像,因此不需要预处理,就能够直接完成低分辨率图像与高分辨率图像之间端到端的映射关系.在不同模型的基准数据集上的实验表明,MMRCNN在峰值信噪比、结构相似性和视觉效果方面均有所提升.},
	language = {chi},
	number = {3},
	journal = {Journal of Zhengzhou University(Natural Science Edition)},
	author = {王知人 and 谷昊晟 and 任福全 and 史紫腾 and 王瑞},
	year = {2020},
	note = {ISBN: 1671-6841
Type: 10.13705/j.issn.1671-6841.2019554
WANG Zhiren
GU Haosheng
REN Fuquan
SHI Ziteng
WANG Rui},
	keywords = {超分辨; 深度学习; 多层映射; 残差学习},
	pages = {42--48},
}

@inproceedings{anderson_boinc_2004,
	title = {{BOINC}: {A} {System} for {Public}-{Resource} {Computing} and {Storage}.},
	url = {https://doi.org/10.1109/GRID.2004.14},
	doi = {10.1109/GRID.2004.14},
	booktitle = {5th {International} {Workshop} on {Grid} {Computing} ({GRID} 2004), 8 {November} 2004, {Pittsburgh}, {PA}, {USA}, {Proceedings}},
	author = {Anderson, David P.},
	year = {2004},
	pages = {4--10},
}

@inproceedings{brown_language_2020,
	title = {Language {Models} are {Few}-{Shot} {Learners}},
	volume = {33},
	url = {https://proceedings.neurips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html},
	abstract = {We demonstrate that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even becoming competitive with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting.  For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model.  GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks. We also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora.},
	urldate = {2022-04-20},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel and Wu, Jeffrey and Winter, Clemens and Hesse, Chris and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
	year = {2020},
	pages = {1877--1901},
	file = {Full Text PDF:C\:\\Users\\86159\\Zotero\\storage\\2U4M5VVN\\Brown 等。 - 2020 - Language Models are Few-Shot Learners.pdf:application/pdf},
}

@inproceedings{jia_beyond_2019,
	title = {Beyond {Data} and {Model} {Parallelism} for {Deep} {Neural} {Networks}.},
	volume = {1},
	url = {https://proceedings.mlsys.org/paper/2019/file/c74d97b01eae257e44aa9d5bade97baf-Paper.pdf},
	booktitle = {Proceedings of {Machine} {Learning} and {Systems}},
	author = {Jia, Zhihao and Zaharia, Matei and Aiken, Alex},
	editor = {Talwalkar, A. and Smith, V. and Zaharia, M.},
	year = {2019},
	pages = {1--13},
}
